VQA models tend to fail on sufficiently novel instances

VQA models tend to regurgitate answers seen during training

VQA models converge on a predicted answer after ‘listening’ to just half the question

VQA models do not change their answers across images

Today’s VQA models –
Driven by language priors in training data. Similar priors in train and test
Lack sufficient image grounding 
Lack compositionality
Memorization of priors does not hurt as much. Problematic for benchmarking progress
