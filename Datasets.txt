1. Visual Turing Test [Geman et al., PNAS 2014] http://www.pnas.org/content/112/12/3618.full.pdf

2. DAQUAR [Malinowski & Fritz, NIPS 2014] https://arxiv.org/pdf/1410.0210.pdf
12.5K QA pairs on ~1.5K images
Closed world answers – basic colors, numbers, objects

3. COCO-QA [Ren et al., NIPS 2015] https://arxiv.org/pdf/1505.02074.pdf
118K QA pairs on ~123K images
Questions and answers generated automatically from image captions
4 types of questions: object, color, number and location

4. FM-IQA [Gao et al., NIPS 2015] https://arxiv.org/pdf/1505.05612.pdf

5. Visual7W [Zhu et al., CVPR 2016] https://arxiv.org/pdf/1511.03416.pdf
328K QA pairs on ~47K images
No yes/no questions
Two types of tasks – telling and pointing
Bounding box annotations for object mentions in QA

6. Visual Genome [Krishna et al., IJCV 2016] https://link.springer.com/content/pdf/10.1007%2Fs11263-016-0981-7.pdf

7. CLEVR [Johnson et al., CVPR 2017] https://arxiv.org/pdf/1612.06890.pdf
865K QA pairs on ~100K images
Synthetic images, questions, and answers
Questions require visual reasoning – attribute identification, comparison, counting, spatial reasoning
Longer questions

8. VQA v2.0 [Goyal et al., CVPR 2017] https://arxiv.org/pdf/1612.00837.pdf
For VQA 1.0, Current machine performance around 65-70%; Human performance at 83%

9. 1 million Full Sentence VQA: http://www.mi.t.u-tokyo.ac.jp/static/projects/fsvqa/
Andrew Shin, Yoshitaka Ushiku, Tatsuya Harada, ”The Color Of The Cat Is Gray: 1 Million Full-Sentences Visual Question Answering (FSVQA)” arXiv, 2016.

10. VQA under changing language priors http://sunw.csail.mit.edu/abstract/vqa-prior.pdf

11. Compositional VQA https://arxiv.org/pdf/1704.08243.pdf
The ability to answer questions about unseen compositions of seen concepts

12. MovieQA http://movieqa.cs.toronto.edu/home/
15,000 multiple choice question answers obtained from over 400 movies and features high semantic diversity. Each question comes with a set of five highly plausible answers; only one of which is correct.


