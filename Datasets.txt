Visual Turing Test [Geman et al., PNAS 2014] http://www.pnas.org/content/112/12/3618.full.pdf

DAQUAR [Malinowski & Fritz, NIPS 2014] https://arxiv.org/pdf/1410.0210.pdf
12.5K QA pairs on ~1.5K images
Closed world answers – basic colors, numbers, objects

COCO-QA [Ren et al., NIPS 2015] https://arxiv.org/pdf/1505.02074.pdf
118K QA pairs on ~123K images
Questions and answers generated automatically from image captions
4 types of questions: object, color, number and location

FM-IQA [Gao et al., NIPS 2015] https://arxiv.org/pdf/1505.05612.pdf

Visual7W [Zhu et al., CVPR 2016] https://arxiv.org/pdf/1511.03416.pdf
328K QA pairs on ~47K images
No yes/no questions
Two types of tasks – telling and pointing
Bounding box annotations for object mentions in QA

Visual Genome [Krishna et al., IJCV 2016] https://link.springer.com/content/pdf/10.1007%2Fs11263-016-0981-7.pdf

CLEVR [Johnson et al., CVPR 2017] https://arxiv.org/pdf/1612.06890.pdf
865K QA pairs on ~100K images
Synthetic images, questions, and answers
Questions require visual reasoning – attribute identification, comparison, counting, spatial reasoning
Longer questions

VQA v2.0 [Goyal et al., CVPR 2017] https://arxiv.org/pdf/1612.00837.pdf
For VQA 1.0, Current machine performance around 65-70%; Human performance at 83%
