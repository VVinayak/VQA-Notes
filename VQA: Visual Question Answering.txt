https://computing.ece.vt.edu/~aish/VQA_ICCV2015.pdf

Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, Devi Parikh
{santol, aish, jiasenlu, dbatra, parikh}@vt.edu 
{memitc, larryz}@microsoft.com

ABSTRACT:
Get natural language answer from natural language question on an image
Questions can target complex and parts of an image => more complicated than captioning
VQA is amenable to automatic evaluation since many open-ended answers contain few words or a fixed set of answers
Contribution: VQA Dataset(∼0.25M images, ∼0.76M questions and ∼10M answers)

Introduction:
Involves fine-grained recognition, commonsense reasoning, object detection, activity recognition, knowledge base reasoning
Can give open ended answer or MCQ task to fill in blanks
Dataset: i) MSCOCO (204721 images depicting diverse/complex scenes effective for eliciting compelling, diverse questions
         ii) 50000 abstract scenes (helps with the high level reasoning needed by VQA)
         iii) 760k questions and 1 million answers based on the images
         
Related Work:
M. Malinowski and M. Fritz. A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input. NIPS, 2014
only considers questions whose answers come from a predefined closed world of 16 basic colors or 894 object categories

D. Geman, S. Geman, N. Hallonquist, and L. Younes. A Visual Turing Test for Computer Vision Systems. In PNAS, 2014.
considers questions generated from templates from a fixed vocabulary of objects, attributes, relationships between objects etc

K. Tu, M. Meng, M. W. Lee, T. E. Choe, and S. C. Zhu. Joint Video and Text Parsing for Understanding Events and Answering Queries. IEEE
MultiMedia, 2014
studied joint parsing of videos and corresponding text to answer queries on two datasets containing 15 video clips each

J. P. Bigham, C. Jayant, H. Ji, G. Little, A. Miller, R. C. Miller, R. Miller, A. Tatarowicz, B. White, S. White, and T. Yeh. VizWiz: Nearly Realtime Answers to Visual Questions. In User Interface Software and Technology, 2010
uses crowdsourced workers to answer questions about visual content asked by visually-impaired users.

M. Malinowski, M. Rohrbach, and M. Fritz. Ask your neurons: A neuralbased approach to answering questions about images. In ICCV, 2015.
LSTM question representation is conditioned on the CNN image features at each time step; final LSTM hidden state is used to sequentially decode the answer phrase

Contribution: LSTM question representation and CNN image features computed independently, fused later via element-wise multiplication, and then passed through fully connected layers to generate softmax distribution over output answer classes

X. Lin and D. Parikh. Don’t just listen, use your imagination: Leveraging visual common sense for non-visual tasks. In CVPR, 2015
generates abstract scenes to capture visual common sense relevant to answering (purely textual) fill-in the-blank and visual paraphrasing questions.

F. Sadeghi, S. K. Kumar Divvala, and A. Farhadi. Viske: Visual knowledge extraction and question answering by visual verification of
relation phrases. In CVPR, 2015
R. Vendantam, X. Lin, T. Batra, C. L. Zitnick, and D. Parikh. Learning common sense through visual abstraction. In ICCV, 2015.]
use visual information to assess the plausibility of common sense assertions.

L. Yu, E. Park, AC Berg, and T. L. Berg. Visual madlibs: Fill-in-theblank description generation and question answering. In ICCV, 2015
introduced a dataset of 10k images and prompted captions that describe specific aspects of a scene (e.g., individual objects, what will happen next)

H. Gao, J. Mao, J. Zhou, Z. Huang, and A. Yuille. Are you talking to a machine? dataset and methods for multilingual image question
answering. In NIPS, 2015
collected questions & answers in Chinese (later translated to English by humans) for COCO images
 
M. Ren, R. Kiros, and R. Zemel. Exploring models and data for image question answering. In NIPS, 2015
automatically generated four types of questions (object, count, color, location) using COCO captions

 J. Weston, A. Bordes, S. Chopra, and T. Mikolov. Towards AIComplete Question Answering: A Set of Prerequisite Toy Tasks. CoRR,
abs/1502.05698, 2015.
synthesized textual descriptions and QA-pairs grounded in a simulation of actors and objects in a fixed set of locations

C. Kong, D. Lin, M. Bansal, R. Urtasun, and S. Fidler. What Are You Talking About? Text-to-Image Coreference. In CVPR, 2014
V. Ramanathan, A. Joulin, P. Liang, and L. Fei-Fei. Linking People with “Their” Names using Coreference Resolution. In ECCV, 2014.
Coreference resolution

S Kazemzadeh, V Ordonez, M Matten, and TL Berg. ReferItGame: Referring to Objects in Photographs of Natural Scenes In EMNLP, 2014
M. Mitchell, K. Van Deemter, and E. Reiter. Generating Expressions that Refer to Visible Objects. In HLT-NAACL, 2013
generating referring expressions for a particular object in an image that would allow a human to identify which object is being referred
